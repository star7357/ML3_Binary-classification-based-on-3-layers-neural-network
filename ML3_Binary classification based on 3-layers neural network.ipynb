{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary classification based on 3 layers neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [ Implementation ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1) Libraries and Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "import torch\n",
    "import math\n",
    "import os\n",
    "\n",
    "# Global Variables\n",
    "train_data_path = './horse-or-human/train'\n",
    "validation_data_path = './horse-or-human/validation'\n",
    "\n",
    "layer_dims = [10000,50,10,1]     # number of units(Neurons) in each layer\n",
    "learning_rate = 0.02            # step size per each epoch (iteration)\n",
    "threshold = 0.1                   # minimum of cost\n",
    "max_epoch = 2000                  # maximum number of epoch (iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_inputs(image_path) :\n",
    "    transform = transforms.Compose([transforms.Grayscale(),transforms.ToTensor(),])\n",
    "    # the code transforms.Grayscale() is for changing the size [3,100,100] to [1, 100, 100]\n",
    "    # (notice : [channel, height, width] )\n",
    "    image_set = torchvision.datasets.ImageFolder(root=image_path, transform=transform)\n",
    "    loader = torch.utils.data.DataLoader(image_set, batch_size=1, shuffle=False, num_workers=1)  \n",
    "\n",
    "    for i,data in enumerate(loader) :\n",
    "        image, label = data\n",
    "        image = image.view(10000,1)\n",
    "        label = label.view(1,1).type(torch.FloatTensor)\n",
    "        \n",
    "        if i == 0 :\n",
    "            images = image\n",
    "            labels = label\n",
    "        else :\n",
    "            images = torch.cat((images,image),dim = 1)\n",
    "            labels = torch.cat((labels,label),dim = 1)\n",
    "    \n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2) Generate Input matrix X and Output vector Y from training and Validation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,Y = initialize_inputs(train_data_path)\n",
    "t_X, t_Y = initialize_inputs(validation_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (3) Implementation - Activation / Parameters / Costs Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(layer_dims) :\n",
    "    parameters = dict()\n",
    "    L = len(layer_dims)    # number of layers in the NN\n",
    "    \n",
    "    for l in range(1,L) :\n",
    "        parameters['W' + str(l)] = torch.randn(layer_dims[l],layer_dims[l-1]) * 0.01\n",
    "        parameters['b' + str(l)] = torch.zeros(layer_dims[l],1)\n",
    "    return parameters\n",
    "\n",
    "def sigmoid(Z) :\n",
    "    A = 1 / (1 + torch.exp(-Z))\n",
    "    cache = Z\n",
    "    return A, cache\n",
    "\n",
    "def ReLU(Z) :\n",
    "    zeros = torch.zeros(Z.size())\n",
    "    A = torch.max(Z,zeros)\n",
    "    cache = Z\n",
    "    return A, cache\n",
    "\n",
    "def cost_computation(A, Y) :\n",
    "    cost = ((-torch.mm(Y,torch.log(A).t()) - torch.mm(1-Y,torch.log(1-A).t())) / A.size()[1]).item()\n",
    "    return cost\n",
    "\n",
    "def update_parameters(parameters, gradients, learning_rate) :\n",
    "    L = len(parameters) // 2 \n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] -= learning_rate * gradients[\"dW\" + str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] -= learning_rate * gradients[\"db\" + str(l+1)]      \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (4) Implementation - Forward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_compute_Z(A,W,b) :\n",
    "    Z = torch.mm(W,A) + b\n",
    "    cache = (A,W,b)\n",
    "    \n",
    "    return Z, cache\n",
    "\n",
    "def forward_compute_activation(A_prev,W,b,activation) :\n",
    "    \n",
    "    Z, parameters_cache = forward_compute_Z(A_prev,W,b)\n",
    "    \n",
    "    if activation == \"sigmoid\" :\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "    elif activation == \"ReLU\" :\n",
    "        A, activation_cache = ReLU(Z)\n",
    "\n",
    "    cache = (parameters_cache, activation_cache)    # cache = ((A_prev,W,b), Z)\n",
    "    return A, cache\n",
    "\n",
    "def forward_propagation(X, parameters) :\n",
    "    \n",
    "    caches = []                # contains Z and (A,W,b) for each layers for backward propagation\n",
    "                               # [0 ~ L-2] : ReLU(Matrices), [L-1] : Sigmoid(Vectors)\n",
    "    A = X                      # initialize A as input X\n",
    "    L = len(parameters) // 2   # number of layers in NN : the reason divide length by 2 is \n",
    "                               # because it contains W and b for each layer seperately \n",
    "                               # as parameters['W_l'] and parameters['b_l']\n",
    "    # Forward propagation through hidden layers (1 <= l < L) - Use ReLU as activation function\n",
    "    for l in range(1,L) :      \n",
    "        A_prev = A\n",
    "        W = parameters['W' + str(l)]\n",
    "        b = parameters['b' + str(l)]\n",
    "        A, cache = forward_compute_activation(A_prev, W,b, \"ReLU\")\n",
    "        (A_prev,W,b), Z = cache\n",
    "#        print(\"<<Layer %d>>\" % l)\n",
    "#        print(A[1])\n",
    "#        print(\"A :\", A[1], \"\\nW :\", W[1], \"\\nb :\", b[1], \"\\nZ :\",Z[1])\n",
    "#        print(\"A_prev :\",A_prev.size(), \", W :\", W.size(), \", b :\", b.size(), \", Z :\", Z.size())\n",
    "        caches.append(cache)\n",
    "\n",
    "    # Forward propagation at the output layer - Use sigmoid as activation function\n",
    "    AL, cache = forward_compute_activation(A, parameters['W' + str(L)], parameters['b' + str(L)], \"sigmoid\")\n",
    "#    (A_prev,W,b), Z = cache\n",
    "#    print(\"A_prev :\",A_prev.size(), \", W :\", W.size(), \", b :\", b.size(), \", Z :\", Z.size())\n",
    "#    print(\"<<Layer %d>>\" % L)\n",
    "#    print(AL)\n",
    "#    print(AL)\n",
    "#    print(parameters['W' + str(L)],\"\\n\", parameters['b' + str(L)])\n",
    "\n",
    "    caches.append(cache)\n",
    "            \n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (5) Implementation - Backward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_compute_parameters(dA, cache, activation) :\n",
    "    (A_prev,W,b), Z = cache         # cache contains (parameter_cache, activation_cache)\n",
    "                                    # parameter_cache is (A_prev, W, b) and activation_cache is Z\n",
    "    m = A_prev.size()[1]            # A_prev.size() = (layer_dims[l-2], # of examples)\n",
    "\n",
    "#    print(\"backward_compute_parameters\")\n",
    "#    print(\"A_prev :\",A_prev.size(), \", W :\", W.size(), \", b :\", b.size(), \", Z :\", Z.size())\n",
    "    \n",
    "    if activation == \"sigmoid\" :\n",
    "        A, _t = sigmoid(Z)          # A = sigmoid(Z), sigmoid`(Z) = A(1-A)\n",
    "        dZ = dA * A * (1-A)         # dZ = dA * g`(Z), g(Z) = sigmoid(Z)\n",
    "        #print(dZ)\n",
    "\n",
    "    elif activation == \"ReLU\" :\n",
    "        dZ = dA.clone().detach()    # g(Z) = ReLU(Z), g`(Z) = 0 for Z < 0 g`(Z) = Z for Z >= 0\n",
    "#        print(\"dZ : \\n\",dZ.size())\n",
    "#        print(\"Z : \\n\", Z.size())\n",
    "\n",
    "        dZ[Z <= 0] = 0              # dZ = dA * g`(Z) ==> dZ = 0 for Z <= 0 and Z for Z >= 0\n",
    "        \n",
    "    dW = torch.mm(dZ,A_prev.t()) / m\n",
    "    db = torch.sum(dZ) / m\n",
    "    dA_prev = torch.mm(W.t(), dZ)\n",
    "#    print(\"\\ndA_prev : \",dA_prev.size())\n",
    "    #print(db)\n",
    "    return dA_prev, dW, db\n",
    "\n",
    "def backward_propagation(AL, Y, caches) :\n",
    "    gradients = dict()       # contains gradients(dA, dW, db) for each epoch (iteration)\n",
    "    L = len(caches)          # number of layers\n",
    "    m = AL.size()[1]         # number of examples (AL.size() = (layer_dims[L-1], # of examples)))\n",
    "    Y = torch.repeat_interleave(Y,AL.size()[0], dim = 0)\n",
    "\n",
    "    dAL = - torch.div(Y,AL) + torch.div(1-Y, 1-AL)\n",
    "    crnt_cache = caches[L-1]\n",
    "    t1,t2,t3 = backward_compute_parameters(dAL, crnt_cache, \"sigmoid\")\n",
    "    gradients[\"dA\" + str(L)] = t1\n",
    "    gradients[\"dW\" + str(L)] = t2\n",
    "    gradients[\"db\" + str(L)] = t3\n",
    "    \n",
    "    for l in reversed(range(L-1)) :\n",
    "        crnt_cache = caches[l]\n",
    "        t1,t2,t3 = backward_compute_parameters(gradients[\"dA\" + str(l + 2)], crnt_cache, \"ReLU\")\n",
    "        gradients[\"dA\" + str(l + 1)] = t1\n",
    "        gradients[\"dW\" + str(l + 1)] = t2\n",
    "        gradients[\"db\" + str(l + 1)] = t3\n",
    "        \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (6) Implementation - predict function for accuracy calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, y, parameters):\n",
    "    \n",
    "    AL, _t = forward_propagation(X, parameters)\n",
    "    correct = torch.zeros(y.size())\n",
    "    m = AL.size()[1]\n",
    "\n",
    "    prediction = AL > 0.5\n",
    "    correct = prediction == y.type(torch.uint8)\n",
    "    \n",
    "    accuracy = torch.sum(correct).item() / m * 100\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (7) Implementation - 3_layer Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Neural_Network_w_3_layers(X,Y,t_X,t_Y,layer_dims,learning_rate,threshold,max_epoch) :\n",
    "    \n",
    "    costs = []\n",
    "    t_costs = []\n",
    "    parameters = initialize_parameters(layer_dims)\n",
    "    \n",
    "    for epoch in range(0,max_epoch) :\n",
    "#        print(\"========================== [ Epoch : %d ] ==========================\" % epoch)\n",
    "        AL, caches = forward_propagation(X, parameters)\n",
    "        t_AL, _t = forward_propagation(t_X, parameters)\n",
    "        \n",
    "        cost = cost_computation(AL, Y)\n",
    "        t_cost = cost_computation(t_AL, t_Y)\n",
    "        \n",
    "        gradients = backward_propagation(AL, Y, caches)\n",
    "        if epoch % 100 == 0 :\n",
    "            print(\"Cost in %d-th iteration : %.6f\" % (epoch, cost))\n",
    "#            print(gradients.items())\n",
    "\n",
    "        \n",
    "        parameters = update_parameters(parameters, gradients, learning_rate)\n",
    "        costs.append(cost)\n",
    "        t_costs.append(t_cost)\n",
    "    \n",
    "        if cost <= threshold :    break\n",
    "        else :                    pass\n",
    "\n",
    "    return costs,t_costs, parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost in 0-th iteration : 0.693144\n",
      "Cost in 100-th iteration : 0.692910\n",
      "Cost in 200-th iteration : 0.692817\n",
      "Cost in 300-th iteration : 0.692768\n",
      "Cost in 400-th iteration : 0.692726\n",
      "Cost in 500-th iteration : 0.692675\n",
      "Cost in 600-th iteration : 0.692594\n",
      "Cost in 700-th iteration : 0.692454\n",
      "Cost in 800-th iteration : 0.692186\n",
      "Cost in 900-th iteration : 0.691616\n",
      "Cost in 1000-th iteration : 0.690202\n",
      "Cost in 1100-th iteration : 0.685719\n",
      "Cost in 1200-th iteration : 0.666428\n",
      "Cost in 1300-th iteration : 0.594936\n"
     ]
    }
   ],
   "source": [
    "costs, t_costs, parameters = Neural_Network_w_3_layers(X,Y,t_X,t_Y,layer_dims,learning_rate,threshold,max_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train = predict(X, Y, parameters)\n",
    "print(pred_train)\n",
    "\n",
    "pred_test = predict(t_X, t_Y, parameters)\n",
    "print(pred_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([x for x in range(len(costs))],costs)\n",
    "plt.show()\n",
    "plt.plot([x for x in range(len(t_costs))],t_costs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
